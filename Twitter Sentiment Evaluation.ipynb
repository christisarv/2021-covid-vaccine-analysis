{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Sentiment Trends with Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm going to match up the tweets to an assigned sentiment, clean up the text data for modeling, and then run a cluster analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:04:21.877249Z",
     "start_time": "2021-07-18T01:04:21.843213Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7dd3504c366f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:49.715405Z",
     "start_time": "2021-07-18T01:03:49.666539Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c7b94b40a8a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "#pulling in py file that maps contractions to full words\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.731998Z",
     "start_time": "2021-07-18T01:03:02.674Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/twint_scraped_clean.csv',  usecols=['date', 'tweet', 'geo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.734343Z",
     "start_time": "2021-07-18T01:03:02.677Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove data from 12.31 that pulled in\n",
    "df = df[df['date'] >= '2021-01-01'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've read in the data pulled with Twint. Now I'm going to use regex to remove a lot of the stuff in the tweets that I don't want to factor into my analysis, such as links, symbols, and @handles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cleaning Text with Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The regex package will help me get non-informative text out of my data by writing patterns to recognize URLs and handles, and using .sub to remove them. I want to do this before assigning a sentiment score in case it throws anything off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.737467Z",
     "start_time": "2021-07-18T01:03:02.684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating a new clean column to remove URLs and @ mentions\n",
    "df['clean_text'] = df['tweet']\n",
    "\n",
    "#first, replacing this weird character with an apostophe as I will deal with contractions later\n",
    "df['clean_text'] = df['clean_text'].str.replace(\"â€™\", \"'\")\n",
    "\n",
    "#removing links\n",
    "df['clean_text'] = [re.sub(r'(http://[^\"\\s]+)|(@\\w+)', '', tweet) for tweet in df['clean_text']]\n",
    "df['clean_text'] = [re.sub(r'(https://[^\"\\s]+)|(@\\w+)', '', tweet) for tweet in df['clean_text']]\n",
    "\n",
    "#removing handles\n",
    "df['clean_text'] = [re.sub(r'@[^\\s]+', '', tweet) for tweet in df['clean_text']]\n",
    "\n",
    "#removing hashtag symbols but keeping the words\n",
    "df['clean_text'] = [re.sub(r\"#\", \" \", tweet) for tweet in df['clean_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Much better, I can now focus on the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.740362Z",
     "start_time": "2021-07-18T01:03:02.689Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assinging a Sentiment with Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'm using the Textblob library which can run a polarity score to analyze the text content to determine how negative or positive it is on a scale. While this capability isn't perfect, it can do a great job of assigning sentiment across large datasets quickly.\n",
    "\n",
    "After creating a function to score the text, I am adding a column to display the score and another column to classify that score based on Textblob's guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.741933Z",
     "start_time": "2021-07-18T01:03:02.694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    assign tweet a polarity score between -1, 1\n",
    "    \"\"\"\n",
    "    \n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.744522Z",
     "start_time": "2021-07-18T01:03:02.698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new column to display scores\n",
    "df['tb_polarity'] = df['clean_text'].apply(get_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.747932Z",
     "start_time": "2021-07-18T01:03:02.702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new column to label sentiment based on score\n",
    "df['tb_sentiment'] = ''\n",
    "df.loc[df.tb_polarity > 0, 'tb_sentiment'] = 'positive'\n",
    "df.loc[df.tb_polarity == 0, 'tb_sentiment'] = 'neutral'\n",
    "df.loc[df.tb_polarity < 0, 'tb_sentiment'] = 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While this method is effective, I can see from a preview that it does have a hard time recognizing negativity, which will be an important caveat for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.751455Z",
     "start_time": "2021-07-18T01:03:02.708Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all my data is labeled, I'm going to look into some trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.753282Z",
     "start_time": "2021-07-18T01:03:02.713Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.tb_sentiment.value_counts().plot(kind='bar',title=\"Textblob Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T02:27:47.425854Z",
     "start_time": "2021-07-01T02:27:47.414419Z"
    }
   },
   "source": [
    "Overall, sentiment is mostly positive, which is consistent with the preview above. I'll need to factor in that Textblob is not necessarily 100% accurate, and many times interpretation can be subjective, especially when the tweet itself is negative but the overall opinion toward vaccines is conveyed as positive. \n",
    "\n",
    "To look deeper into positive vs negative, I'm going to show the most frequent words and visualize with a word cloud. This will require further text cleaning to make sure I'm getting the most relevant results. I'll start by separating positive, negative, and neutral data into separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:03:02.755285Z",
     "start_time": "2021-07-18T01:03:02.717Z"
    }
   },
   "outputs": [],
   "source": [
    "negatives = df[df['tb_sentiment'] == 'negative']\n",
    "positives = df[df['tb_sentiment'] == 'positive']\n",
    "neutral = df[df['tb_sentiment'] == 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first ran the text analysis I saw that contractions were being split and strings like \"n't\" were showing up as frequent words. To solve this I am using a py file and function imported from [this repository](https://github.com/dipanjanS/practical-machine-learning-with-python) in order to map contractions to full words. After defining the fuction and applying it across all the text data, I can see the results in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.813877Z",
     "start_time": "2021-07-18T01:02:50.249Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using the contractions.py file cloned to this repository, \n",
    "    defines a regex pattern and uses it to map contractions to\n",
    "    full words and replace them in the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.815908Z",
     "start_time": "2021-07-18T01:02:50.252Z"
    }
   },
   "outputs": [],
   "source": [
    "positives['clean_text'] = list(map(expand_contractions, positives['clean_text']))\n",
    "negatives['clean_text'] = list(map(expand_contractions, negatives['clean_text']))\n",
    "neutral['clean_text'] = list(map(expand_contractions, neutral['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.818347Z",
     "start_time": "2021-07-18T01:02:50.255Z"
    }
   },
   "outputs": [],
   "source": [
    "positives['clean_text'] #shows expanded contraction example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I am using NLTK's stopwords list to remove words that don't add meaning (i.e. 'is', 'who', 'for') and adding punctuation, as well as the term 'vaccine' as this should be in every tweet. I can see that Twint is also pulling in some things as weird characters so I'm removing those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.835211Z",
     "start_time": "2021-07-18T01:02:50.259Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += string.punctuation\n",
    "punct_list = [\"''\", '\"\"', '...', '``', 'amp', \"'\", '\"', \"`\"]\n",
    "stopwords_list += punct_list\n",
    "stopwords_list.extend(['vaccine', 'vaccines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'm going to incorporate this into a tokenize function that will map contractions, lowercase words, and remove stopwords and symbols all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.841743Z",
     "start_time": "2021-07-18T01:02:50.263Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preps text data for processing by passing\n",
    "    several data cleaning techniques at once:\n",
    "    mapping contractions, lowercasing, and \n",
    "    removing stopwords/punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    words_lower = [word.lower() for word in tokens]\n",
    "    words_stopped = [word for word in words_lower if word not in stopwords_list]\n",
    "    return words_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.844123Z",
     "start_time": "2021-07-18T01:02:50.270Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_data_positive = list(map(tokenize, positives['clean_text']))\n",
    "processed_data_negative = list(map(tokenize, negatives['clean_text']))\n",
    "processed_data_neutral = list(map(tokenize, neutral['clean_text']))\n",
    "processed_data_positive[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize word clouds, I'm compiling a frequency distribution of most common words in positive and negative labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.847425Z",
     "start_time": "2021-07-18T01:02:50.273Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_tweet_concat = []\n",
    "for tweet in processed_data_positive:\n",
    "    pos_tweet_concat += tweet\n",
    "    \n",
    "pos_tweet_freqdist = FreqDist(pos_tweet_concat)\n",
    "# pos_tweet_freqdist.most_common(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.850468Z",
     "start_time": "2021-07-18T01:02:50.276Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_tweet_concat = []\n",
    "for tweet in processed_data_negative:\n",
    "    neg_tweet_concat += tweet\n",
    "    \n",
    "neg_tweet_freqdist = FreqDist(neg_tweet_concat)\n",
    "# neg_tweet_freqdist.most_common(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.854530Z",
     "start_time": "2021-07-18T01:02:50.279Z"
    }
   },
   "outputs": [],
   "source": [
    "p_wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "p_wc.generate_from_frequencies(pos_tweet_freqdist)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(p_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=1, y=1)\n",
    "plt.title('Positive Word Cloud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.857523Z",
     "start_time": "2021-07-18T01:02:50.283Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "n_wc.generate_from_frequencies(neg_tweet_freqdist)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(n_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=1, y=1)\n",
    "plt.title('Negative Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word clouds give some indication of what positive and negative sentiment revolves around: with the positive word cloud there are a lot of terms around availability (today, available, new, appointment, first/second dose), where in the second there are terms that show concern about being able to get a shot as well as the effects: (work, effects, sick, need, time).\n",
    "\n",
    "Next, I want to create a dataframe that shows how sentiment trends over time, which I'll use to correlate with the vaccination rate data. I'll do this by taking the average polarity score over time overall, and then for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.861291Z",
     "start_time": "2021-07-18T01:02:50.287Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_over_time = df[['date', 'tb_polarity']].sort_values(by ='date')\n",
    "sent_over_time['date'] =pd.to_datetime(sent_over_time['date'])\n",
    "sent_over_time = sent_over_time.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.864301Z",
     "start_time": "2021-07-18T01:02:50.291Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.plot(sent_over_time)\n",
    "plt.title(\"Polarity Score Over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the trend looks pretty random with a few dramatic spikes. Let's take a closer look at different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.867941Z",
     "start_time": "2021-07-18T01:02:50.294Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_by_location(df, location):\n",
    "    \n",
    "    \"\"\"\n",
    "    creates time series of average polarity score \n",
    "    over time specific to each location passed in.\n",
    "    \"\"\"\n",
    "    \n",
    "    location_df = df[df['geo'] == location]\n",
    "    return location_df[['date', 'tb_polarity']].sort_values(by ='date').groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.870378Z",
     "start_time": "2021-07-18T01:02:50.297Z"
    }
   },
   "outputs": [],
   "source": [
    "hou_sent = sent_by_location(df, 'houston')\n",
    "chi_sent = sent_by_location(df, 'chicago')\n",
    "nyc_sent = sent_by_location(df, 'nyc')\n",
    "la_sent = sent_by_location(df, 'la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.873370Z",
     "start_time": "2021-07-18T01:02:50.300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot(hou_sent, label='Houston')  \n",
    "plt.plot(chi_sent, label='Chicago')  \n",
    "plt.plot(nyc_sent, label='New York City')\n",
    "plt.plot(la_sent, label='Los Angeles')\n",
    "plt.xlabel('Date')  \n",
    "plt.ylabel('Sentiment Score') \n",
    "plt.title(\"Vaccine Sentiment over Time\")  \n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.876409Z",
     "start_time": "2021-07-18T01:02:50.305Z"
    }
   },
   "outputs": [],
   "source": [
    "#exporting these dataframes to use in my other notebook\n",
    "\n",
    "# hou_sent.to_csv('data/houston_sentiment.csv')\n",
    "# chi_sent.to_csv('data/chicago_sentiment.csv')\n",
    "# nyc_sent.to_csv('data/nyc_sentiment.csv')\n",
    "# la_sent.to_csv('data/la_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using unsupervised learning techniques to do a clustering analysis to explore topics of conversation within my data. I'll look at positive vs negative vs neutral topics to see if there is any clear differentiation.\n",
    "\n",
    "I'm using a Latent Dirichlet Allocation model, a probabilistic model that sees a document as a mix of topics, and a maps each word to a topic. \n",
    "\n",
    "This LDA model comes from Gensim and requires an input number of topics, as well a the body of text and a dictionary of word IDs and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.882304Z",
     "start_time": "2021-07-18T01:02:50.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "pos_id2word = corpora.Dictionary(processed_data_positive)\n",
    "# Create Corpus\n",
    "pos_texts = processed_data_positive\n",
    "# Term Document Frequency\n",
    "pos_corpus = [pos_id2word.doc2bow(text) for text in pos_texts]\n",
    "# View the result\n",
    "print(pos_corpus[:1][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.885253Z",
     "start_time": "2021-07-18T01:02:50.314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "pos_lda_model = gensim.models.LdaMulticore(corpus=pos_corpus,\n",
    "                                       id2word=pos_id2word,\n",
    "                                       num_topics=3)\n",
    "# Print the keywords in the topics\n",
    "pprint(pos_lda_model.print_topics())\n",
    "doc_lda = pos_lda_model[pos_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.887247Z",
     "start_time": "2021-07-18T01:02:50.319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.890695Z",
     "start_time": "2021-07-18T01:02:50.326Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_LDAvis_prepared = pyLDAvis.gensim.prepare(pos_lda_model, pos_corpus, pos_id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.893688Z",
     "start_time": "2021-07-18T01:02:50.334Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos_LDAvis_prepared.pickle', 'wb') as f:\n",
    "        pickle.dump(pos_LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open('pos_LDAvis_prepared.pickle', 'rb') as f:\n",
    "    pos_LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "    # pyLDAvis.save_html(LDAvis_prepared)\n",
    "pos_LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.896187Z",
     "start_time": "2021-07-18T01:02:50.341Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_id2word = corpora.Dictionary(processed_data_negative)\n",
    "\n",
    "neg_texts = processed_data_negative\n",
    "\n",
    "neg_corpus = [neg_id2word.doc2bow(text) for text in neg_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.898337Z",
     "start_time": "2021-07-18T01:02:50.344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "neg_lda_model = gensim.models.LdaMulticore(corpus=neg_corpus,\n",
    "                                       id2word=neg_id2word,\n",
    "                                       num_topics=3)\n",
    "# Print the keywords in the topics\n",
    "# pprint(lda_model.print_topics())\n",
    "neg_doc_lda = neg_lda_model[neg_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.900332Z",
     "start_time": "2021-07-18T01:02:50.347Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_LDAvis_prepared = pyLDAvis.gensim.prepare(neg_lda_model, neg_corpus, neg_id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-18T01:02:52.902535Z",
     "start_time": "2021-07-18T01:02:50.354Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('LDAvis_prepared.pickle', 'wb') as f:\n",
    "        pickle.dump(neg_LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open('LDAvis_prepared.pickle', 'rb') as f:\n",
    "    neg_LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "    # pyLDAvis.save_html(LDAvis_prepared)\n",
    "neg_LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
